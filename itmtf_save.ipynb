{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time slices with docs: 123\n",
      "Number of time slices: 123\n",
      "Number of time vocab: 12517\n",
      "Number of documents: 2673\n",
      "Number of unique tokens: 12517\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# loads the pre-processed variables\n",
    "%run load_helper.ipynb\n",
    "#\n",
    "\n",
    "# ##################################################\n",
    "#\n",
    "# SET UP PARAMETERS\n",
    "# next 2 sections \n",
    "# (3rd is opitional if running the \"classic\" algorithm)\n",
    "#\n",
    "# then hit run\n",
    "#\n",
    "# ##################################################\n",
    "\n",
    "\n",
    "# ##################################################\n",
    "#\n",
    "# pick a baseline\n",
    "#        baselines = [\"10topics\", \"15topics\", \"20topics\", \"25topics\", \"30topics\"]\n",
    "#\n",
    "# name your run (used to store the iterations); models will be saved myrun1.sav myrun2.save etc\n",
    "#\n",
    "# ##################################################\n",
    "mybaseline = \"30baseline2\"\n",
    "runname = \"Testcleanupclassic\"\n",
    "#runname = \"D75T30Improved20\"\n",
    "\n",
    "# ##################################################\n",
    "#\n",
    "# set your global parameters\n",
    "#\n",
    "# NOTE if you want to run the algorithm with the paper\n",
    "#      also set parameters in next section\n",
    "#\n",
    "# ##################################################\n",
    "\n",
    "# below are default params that can be played with\n",
    "lda_decay = float(.5)      # how much the prior influences the iteration 0 - 1 \n",
    "                           #      mathmatically anything less than .5 is not guartenteed to converge\n",
    "                           #      however we have tuned the model to work down to .001 (and possibly lower)\n",
    "                           # if you do set this too low, Gensim will display warnings\n",
    "            \n",
    "num_iterations = 14  # NOTE this is in addition to baseline so if you want 15 iterations set this to 14\n",
    "                     # if you are using the \"improved algorithm\" set this to about 14\n",
    "\n",
    "# lag of 5 is mentioned in the paper, and seems to work twith trial runs\n",
    "the_lag = 5\n",
    "\n",
    "# ##################################################\n",
    "#\n",
    "# OPTIONAL\n",
    "# if you want to run an iteration with the paper's algorithm (with splitting and buffers)\n",
    "# set your parameters \"classical\" parameters\n",
    "#\n",
    "# ##################################################\n",
    "\n",
    "# don't forget to comment these out if you want to go back to \"improved\" version\n",
    "classical = 'y'      # uncomment out this line to run the classical algorithm\n",
    "num_iterations = 4   # if you are going to use the algorthim in the paper (with splitting) set to around 4\n",
    "lda_decay = float(.001)      # how much the prior influences the iteration 0 - 1 \n",
    "\n",
    "num_buffers = 0    # how many buffers to add each iteration\n",
    "drop_percent = float(.95)         # as indicited in the paper, drop below .95 percent\n",
    "low_threshold = float(.05)        # threshold for the p-values .05 is pretty much expected\n",
    "ignore_little_counts = float(.2)  # if pos/neg words dominate, ignore the other topic\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##################################################\n",
    "#\n",
    "# The ITMFT algorithm\n",
    "#\n",
    "# ##################################################\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "#\n",
    "# load the baseline selected in the parameters above\n",
    "#\n",
    "\n",
    "file_name = save_path + mybaseline + \".sav\"\n",
    "model = LdaModel.load(file_name)\n",
    "    \n",
    "topics = model.get_topics().copy()\n",
    "topics = topics.copy()\n",
    "num_topics = len(topics)\n",
    "print(\"Number of Topics = \", num_topics)\n",
    "\n",
    "run_purity = []\n",
    "run_confidence = []\n",
    "\n",
    "mostsigtopics = []\n",
    "mostsigtopicwords = []\n",
    "mostsigconf = float(2.0)\n",
    "\n",
    "while iteration < num_iterations  :\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"\\nIteration start time = \", current_time)\n",
    "    \n",
    "    # \n",
    "    # run either the \"classical\" or the \"improved\"\n",
    "    #\n",
    "    if classical == 'y'  :\n",
    "        %run itmtf_withsplit.ipynb \n",
    "    else :\n",
    "        %run itmtf_improved.ipynb \n",
    "    \n",
    "    #\n",
    "    # run the model\n",
    "    #\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=tokentoword,\n",
    "        chunksize=chunksize,\n",
    "        alpha='auto',               \n",
    "        eta=newtopics,                 # preset topic/word\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,         # added buffer topics\n",
    "        passes=passes,\n",
    "        decay = lda_decay,\n",
    "        eval_every=eval_every\n",
    "    )\n",
    "\n",
    "    topics = model.get_topics().copy()\n",
    "    num_topics = len(topics)\n",
    " \n",
    "    file_name = runname + str(iteration) \n",
    "    path_name = save_path + file_name + \".sav\"\n",
    "    print(\"Model \" + file_name + \" - saved for visualization\")\n",
    "    model.save(path_name )\n",
    "    iteration += 1\n",
    "\n",
    "#\n",
    "# after the iterations are done\n",
    "# run the algorithm once more to gather stats from the last model\n",
    "#\n",
    "if classical == 'y'  :\n",
    "    %run itmtf_withsplit.ipynb \n",
    "else :\n",
    "    %run itmtf_improved.ipynb \n",
    "#\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Run Complete = \", current_time)\n",
    " \n",
    "print (\"Significant Topics\", mostsigtopics)\n",
    "for ii in range (0, len(mostsigtopics)) :\n",
    "    print(\"Top Topic Words: \", mostsigtopics[ii])\n",
    "    words = \" \"\n",
    "    for yy in range (0,10) :\n",
    "        words = words + tokentoword[mostsigtopicwords[ii][yy][0]] + \" \"\n",
    "    print(words)\n",
    "\n",
    "#\n",
    "# save the stats\n",
    "#\n",
    "path_name = save_path + runname + \".sigwords.csv\"\n",
    "fo = open(path_name, \"w\")\n",
    "firstime = 0\n",
    "for ii in range (0, len(mostsigtopics)) :\n",
    "    fo.write(str(mostsigtopics[ii]))\n",
    "    words = \" \"\n",
    "    for yy in range (0,10) :\n",
    "        if pearsoncorr[mostsigtopicwords[ii][yy][0]] > 0 :\n",
    "            words = words + \",B+: \" + tokentoword[mostsigtopicwords[ii][yy][0]]\n",
    "        else :\n",
    "            words = words + \",G+: \" + tokentoword[mostsigtopicwords[ii][yy][0]]\n",
    "    words = words + \"\\n\"\n",
    "    fo.write(words) \n",
    "fo.close()  \n",
    "\n",
    "path_name = save_path + runname + \".confidence.csv\"\n",
    "fo = open(path_name, \"w\")\n",
    "firstime = 0\n",
    "for num in run_confidence :\n",
    "    if firstime == 0 :\n",
    "        fo.write(str(num) )\n",
    "        firstime = 1\n",
    "    else :      \n",
    "        fo.write(\"\\n\" + str(num))                 \n",
    "fo.close()   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to disk.\n",
    "temp_file = datapath(\"it1_model\")\n",
    "model.save(temp_file)\n",
    "model.show_topic(30, 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_file = datapath(\"baseline_model\")\n",
    "#newmodel = LdaModel.load(temp_file)\n",
    "#newmodel.show_topic(29, 50) # will blow up if 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of probs\n",
    "doctopic = {}\n",
    "count = 0\n",
    "for onebag in bow :\n",
    "    doctopic[count] = model.get_document_topics(onebag)\n",
    "    count += 1\n",
    "print ( len(doctopic))\n",
    "\n",
    "# get the doc list for this iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (doctopic[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gensim",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
